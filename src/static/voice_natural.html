<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Leaf & Loaf - Natural Voice Assistant</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #0a0e27;
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
            color: white;
        }
        
        .container {
            max-width: 1000px;
            width: 100%;
        }
        
        .header {
            text-align: center;
            margin-bottom: 40px;
        }
        
        h1 {
            font-size: 48px;
            font-weight: 300;
            margin-bottom: 10px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        
        .subtitle {
            color: #8892b0;
            font-size: 18px;
        }
        
        .main-display {
            background: rgba(17, 25, 58, 0.6);
            backdrop-filter: blur(10px);
            border: 1px solid rgba(102, 126, 234, 0.2);
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
        }
        
        .emotion-indicator {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-bottom: 20px;
            font-size: 14px;
            color: #8892b0;
        }
        
        .emotion-dot {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #4caf50;
            transition: all 0.3s ease;
        }
        
        .emotion-dot.negative { background: #f44336; }
        .emotion-dot.neutral { background: #ffc107; }
        .emotion-dot.positive { background: #4caf50; }
        
        .transcript-display {
            min-height: 100px;
            margin-bottom: 30px;
        }
        
        .current-transcript {
            font-size: 28px;
            line-height: 1.4;
            color: #ccd6f6;
            margin-bottom: 10px;
            min-height: 40px;
        }
        
        .interim-transcript {
            font-size: 20px;
            color: #64748b;
            opacity: 0.7;
            font-style: italic;
        }
        
        .insights-bar {
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        
        .insight-item {
            background: rgba(102, 126, 234, 0.1);
            padding: 10px 20px;
            border-radius: 50px;
            font-size: 14px;
            display: flex;
            align-items: center;
            gap: 8px;
            border: 1px solid rgba(102, 126, 234, 0.3);
        }
        
        .insight-label {
            color: #8892b0;
        }
        
        .insight-value {
            color: #667eea;
            font-weight: 500;
        }
        
        .voice-visualizer {
            height: 120px;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 30px;
            position: relative;
        }
        
        .frequency-bars {
            display: flex;
            align-items: center;
            gap: 4px;
            height: 100%;
        }
        
        .frequency-bar {
            width: 6px;
            background: linear-gradient(to top, #667eea, #764ba2);
            border-radius: 3px;
            transition: height 0.1s ease;
            opacity: 0.8;
        }
        
        .assistant-bubble {
            background: rgba(102, 126, 234, 0.1);
            border: 1px solid rgba(102, 126, 234, 0.3);
            border-radius: 20px;
            padding: 20px 30px;
            margin-bottom: 20px;
            opacity: 0;
            transform: translateY(20px);
            animation: fadeInUp 0.5s ease forwards;
        }
        
        @keyframes fadeInUp {
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        
        .assistant-text {
            font-size: 18px;
            line-height: 1.6;
            color: #ccd6f6;
        }
        
        .products-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 20px;
        }
        
        .product-card {
            background: rgba(17, 25, 58, 0.8);
            border: 1px solid rgba(102, 126, 234, 0.2);
            border-radius: 12px;
            padding: 15px;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .product-card:hover {
            transform: translateY(-2px);
            border-color: rgba(102, 126, 234, 0.5);
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.2);
        }
        
        .product-name {
            font-weight: 500;
            color: #ccd6f6;
            margin-bottom: 5px;
        }
        
        .product-details {
            font-size: 14px;
            color: #8892b0;
        }
        
        .product-price {
            color: #667eea;
            font-weight: bold;
            font-size: 18px;
            margin-top: 5px;
        }
        
        .connection-status {
            position: fixed;
            top: 20px;
            right: 20px;
            display: flex;
            align-items: center;
            gap: 10px;
            background: rgba(17, 25, 58, 0.8);
            padding: 10px 20px;
            border-radius: 50px;
            font-size: 14px;
        }
        
        .status-dot {
            width: 8px;
            height: 8px;
            border-radius: 50%;
            background: #4caf50;
            animation: pulse 2s infinite;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }
        
        .start-button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            border: none;
            color: white;
            padding: 20px 40px;
            font-size: 20px;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            display: block;
            margin: 0 auto;
            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
        }
        
        .start-button:hover {
            transform: translateY(-2px);
            box-shadow: 0 15px 40px rgba(102, 126, 234, 0.4);
        }
        
        .hidden { display: none; }
    </style>
</head>
<body>
    <div class="connection-status">
        <div class="status-dot"></div>
        <span id="connectionStatus">Ready</span>
    </div>
    
    <div class="container">
        <div class="header">
            <h1>Natural Voice Shopping</h1>
            <p class="subtitle">Just speak naturally - I understand context, emotion, and intent</p>
        </div>
        
        <button id="startButton" class="start-button">Start Conversation</button>
        
        <div id="mainInterface" class="hidden">
            <div class="main-display">
                <div class="emotion-indicator">
                    <div id="emotionDot" class="emotion-dot neutral"></div>
                    <span>Detecting: <span id="emotionText">Ready to listen</span></span>
                </div>
                
                <div class="transcript-display">
                    <div id="currentTranscript" class="current-transcript"></div>
                    <div id="interimTranscript" class="interim-transcript"></div>
                </div>
                
                <div id="insightsBar" class="insights-bar hidden">
                    <div class="insight-item">
                        <span class="insight-label">Sentiment:</span>
                        <span id="sentimentValue" class="insight-value">-</span>
                    </div>
                    <div class="insight-item">
                        <span class="insight-label">Intent:</span>
                        <span id="intentValue" class="insight-value">-</span>
                    </div>
                    <div class="insight-item">
                        <span class="insight-label">Urgency:</span>
                        <span id="urgencyValue" class="insight-value">-</span>
                    </div>
                    <div class="insight-item">
                        <span class="insight-label">Clarity:</span>
                        <span id="clarityValue" class="insight-value">-</span>
                    </div>
                </div>
                
                <div class="voice-visualizer">
                    <div id="frequencyBars" class="frequency-bars"></div>
                </div>
            </div>
            
            <div id="assistantResponse"></div>
        </div>
    </div>
    
    <script>
        // Configuration
        const WS_URL = window.location.hostname === 'localhost' 
            ? 'ws://localhost:8080/api/v1/voice-realtime/stream' 
            : 'wss://leafloaf-32905605817.us-central1.run.app/api/v1/voice-realtime/stream';
        
        // State
        let websocket = null;
        let mediaRecorder = null;
        let audioContext = null;
        let analyser = null;
        let isConnected = false;
        let audioQueue = [];
        let isPlaying = false;
        
        // Elements
        const startButton = document.getElementById('startButton');
        const mainInterface = document.getElementById('mainInterface');
        const connectionStatus = document.getElementById('connectionStatus');
        const emotionDot = document.getElementById('emotionDot');
        const emotionText = document.getElementById('emotionText');
        const currentTranscript = document.getElementById('currentTranscript');
        const interimTranscript = document.getElementById('interimTranscript');
        const insightsBar = document.getElementById('insightsBar');
        const assistantResponse = document.getElementById('assistantResponse');
        const frequencyBars = document.getElementById('frequencyBars');
        
        // Create frequency bars
        for (let i = 0; i < 32; i++) {
            const bar = document.createElement('div');
            bar.className = 'frequency-bar';
            bar.style.height = '10px';
            frequencyBars.appendChild(bar);
        }
        
        // Audio visualization
        function setupAudioVisualization(stream) {
            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyser);
            
            analyser.fftSize = 64;
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            
            function draw() {
                requestAnimationFrame(draw);
                analyser.getByteFrequencyData(dataArray);
                
                const bars = frequencyBars.children;
                for (let i = 0; i < bars.length; i++) {
                    const value = dataArray[i] || 0;
                    const height = (value / 255) * 100;
                    bars[i].style.height = `${Math.max(10, height)}px`;
                }
            }
            
            draw();
        }
        
        // Connect to WebSocket
        async function connect() {
            try {
                connectionStatus.textContent = 'Connecting...';
                
                websocket = new WebSocket(WS_URL);
                websocket.binaryType = 'arraybuffer';
                
                websocket.onopen = async () => {
                    console.log('Connected to real-time voice');
                    isConnected = true;
                    connectionStatus.textContent = 'Connected';
                    startButton.classList.add('hidden');
                    mainInterface.classList.remove('hidden');
                    
                    // Start microphone
                    await startMicrophone();
                };
                
                websocket.onmessage = handleMessage;
                
                websocket.onerror = (error) => {
                    console.error('WebSocket error:', error);
                    connectionStatus.textContent = 'Connection error';
                };
                
                websocket.onclose = () => {
                    console.log('Disconnected');
                    disconnect();
                };
                
            } catch (error) {
                console.error('Connection failed:', error);
                connectionStatus.textContent = 'Failed to connect';
            }
        }
        
        // Handle incoming messages
        function handleMessage(event) {
            if (event.data instanceof ArrayBuffer) {
                // Audio data - queue for playback
                audioQueue.push(event.data);
                playNextAudio();
            } else {
                // JSON message
                try {
                    const data = JSON.parse(event.data);
                    
                    switch (data.type) {
                        case 'interim_transcript':
                            interimTranscript.textContent = data.text;
                            break;
                            
                        case 'final_transcript':
                            currentTranscript.textContent = data.text;
                            interimTranscript.textContent = '';
                            
                            // Show insights
                            if (data.insights) {
                                updateInsights(data.insights);
                            }
                            break;
                            
                        case 'assistant_message':
                            showAssistantMessage(data.text, data.emotion, data.products);
                            break;
                            
                        case 'user_speaking':
                            if (data.status === 'started') {
                                emotionText.textContent = 'Listening...';
                            }
                            break;
                    }
                } catch (e) {
                    console.error('Failed to parse message:', e);
                }
            }
        }
        
        // Update insights display
        function updateInsights(insights) {
            insightsBar.classList.remove('hidden');
            
            // Update values
            document.getElementById('sentimentValue').textContent = insights.sentiment || '-';
            document.getElementById('intentValue').textContent = insights.intent || '-';
            document.getElementById('urgencyValue').textContent = 
                insights.urgency ? `${Math.round(insights.urgency * 100)}%` : '-';
            document.getElementById('clarityValue').textContent = 
                insights.clarity ? `${Math.round(insights.clarity * 100)}%` : '-';
            
            // Update emotion indicator
            if (insights.sentiment) {
                emotionDot.className = `emotion-dot ${insights.sentiment}`;
                emotionText.textContent = `Feeling: ${insights.sentiment}`;
            }
        }
        
        // Show assistant message
        function showAssistantMessage(text, emotion, products) {
            const bubble = document.createElement('div');
            bubble.className = 'assistant-bubble';
            
            const textDiv = document.createElement('div');
            textDiv.className = 'assistant-text';
            textDiv.textContent = text;
            bubble.appendChild(textDiv);
            
            if (products && products.length > 0) {
                const grid = document.createElement('div');
                grid.className = 'products-grid';
                
                products.forEach(product => {
                    const card = document.createElement('div');
                    card.className = 'product-card';
                    card.innerHTML = `
                        <div class="product-name">${product.product_name}</div>
                        <div class="product-details">${product.supplier || product.category}</div>
                        <div class="product-price">$${product.price.toFixed(2)}</div>
                    `;
                    card.onclick = () => {
                        // Could add to cart here
                        console.log('Selected:', product);
                    };
                    grid.appendChild(card);
                });
                
                bubble.appendChild(grid);
            }
            
            assistantResponse.innerHTML = '';
            assistantResponse.appendChild(bubble);
        }
        
        // Play audio queue
        async function playNextAudio() {
            if (isPlaying || audioQueue.length === 0) return;
            
            isPlaying = true;
            const audioData = audioQueue.shift();
            
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }
                
                const audioBuffer = await audioContext.decodeAudioData(audioData);
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                
                source.onended = () => {
                    isPlaying = false;
                    playNextAudio();
                };
                
                source.start(0);
            } catch (error) {
                console.error('Audio playback error:', error);
                isPlaying = false;
                playNextAudio();
            }
        }
        
        // Start microphone streaming
        async function startMicrophone() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 16000
                    }
                });
                
                // Setup visualization
                setupAudioVisualization(stream);
                
                // Create MediaRecorder for streaming
                const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
                    ? 'audio/webm;codecs=opus' 
                    : 'audio/webm';
                    
                mediaRecorder = new MediaRecorder(stream, { mimeType });
                
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0 && websocket.readyState === WebSocket.OPEN) {
                        websocket.send(event.data);
                    }
                };
                
                // Start recording in small chunks
                mediaRecorder.start(100);
                
            } catch (error) {
                console.error('Microphone error:', error);
                connectionStatus.textContent = 'Microphone access denied';
                disconnect();
            }
        }
        
        // Disconnect
        function disconnect() {
            isConnected = false;
            
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => track.stop());
            }
            
            if (websocket) {
                websocket.close();
            }
            
            if (audioContext) {
                audioContext.close();
            }
            
            startButton.classList.remove('hidden');
            mainInterface.classList.add('hidden');
            connectionStatus.textContent = 'Disconnected';
        }
        
        // Start button handler
        startButton.addEventListener('click', connect);
        
        // Check for microphone permission
        window.addEventListener('load', async () => {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                stream.getTracks().forEach(track => track.stop());
            } catch (err) {
                connectionStatus.textContent = 'Microphone permission needed';
                startButton.disabled = true;
            }
        });
    </script>
</body>
</html>