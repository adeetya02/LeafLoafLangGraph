<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LeafLoaf Voice Assistant - Google Cloud</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            background: white;
            border-radius: 12px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2d7d46;
            text-align: center;
            margin-bottom: 10px;
        }
        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
        }
        .controls {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-bottom: 30px;
            flex-wrap: wrap;
        }
        button {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s;
        }
        .primary {
            background: #2d7d46;
            color: white;
        }
        .primary:hover {
            background: #246339;
        }
        .primary:disabled {
            background: #ccc;
            cursor: not-allowed;
        }
        .secondary {
            background: #e0e0e0;
            color: #333;
        }
        .secondary:hover {
            background: #d0d0d0;
        }
        .recording {
            background: #dc3545;
            color: white;
            animation: pulse 1.5s infinite;
        }
        @keyframes pulse {
            0% { opacity: 1; }
            50% { opacity: 0.7; }
            100% { opacity: 1; }
        }
        .status {
            text-align: center;
            padding: 10px;
            margin-bottom: 20px;
            border-radius: 8px;
            font-weight: 500;
        }
        .status.connected {
            background: #d4edda;
            color: #155724;
        }
        .status.disconnected {
            background: #f8d7da;
            color: #721c24;
        }
        .status.processing {
            background: #cce5ff;
            color: #004085;
        }
        .language-selector {
            margin-bottom: 20px;
            text-align: center;
        }
        select {
            padding: 8px 16px;
            border-radius: 6px;
            border: 1px solid #ddd;
            font-size: 16px;
        }
        .conversation {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 20px;
            height: 400px;
            overflow-y: auto;
            background: #fafafa;
        }
        .message {
            margin-bottom: 15px;
            padding: 10px 15px;
            border-radius: 8px;
            animation: slideIn 0.3s ease-out;
        }
        @keyframes slideIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
        .user {
            background: #e3f2fd;
            margin-left: 50px;
            border-left: 3px solid #2196f3;
        }
        .assistant {
            background: #e8f5e9;
            margin-right: 50px;
            border-left: 3px solid #4caf50;
        }
        .error {
            background: #ffebee;
            border-left: 3px solid #f44336;
            color: #c62828;
        }
        .transcript {
            font-style: italic;
            color: #666;
            margin-bottom: 5px;
        }
        .metadata {
            font-size: 12px;
            color: #999;
            margin-top: 5px;
        }
        .voice-indicator {
            display: inline-block;
            width: 20px;
            height: 20px;
            margin: 0 5px;
            border-radius: 50%;
            background: #4caf50;
            animation: soundWave 1s infinite;
        }
        @keyframes soundWave {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.2); }
        }
        .text-input {
            display: flex;
            gap: 10px;
            margin-top: 20px;
        }
        .text-input input {
            flex: 1;
            padding: 10px;
            border: 1px solid #ddd;
            border-radius: 6px;
            font-size: 16px;
        }
        .features {
            margin-top: 30px;
            padding: 20px;
            background: #f0f8ff;
            border-radius: 8px;
        }
        .features h3 {
            color: #2d7d46;
            margin-bottom: 10px;
        }
        .features ul {
            margin: 0;
            padding-left: 20px;
        }
        .features li {
            margin-bottom: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è LeafLoaf Voice Assistant</h1>
        <p class="subtitle">Powered by Google Cloud Speech & Voice-Native AI</p>
        
        <div class="status" id="status">Disconnected</div>
        
        <div class="language-selector">
            <label for="language">Language: </label>
            <select id="language" onchange="changeLanguage()">
                <option value="en-US">English (US)</option>
                <option value="en-IN">English (India)</option>
                <option value="es-US">Spanish (US)</option>
                <option value="hi-IN">Hindi</option>
                <option value="zh-CN">Chinese (Mandarin)</option>
                <option value="ko-KR">Korean</option>
                <option value="ja-JP">Japanese</option>
                <option value="vi-VN">Vietnamese</option>
                <option value="ar-SA">Arabic</option>
                <option value="pt-BR">Portuguese (Brazil)</option>
                <option value="bn-IN">Bengali</option>
                <option value="ta-IN">Tamil</option>
                <option value="fr-FR">French</option>
                <option value="de-DE">German</option>
                <option value="it-IT">Italian</option>
                <option value="ru-RU">Russian</option>
                <option value="th-TH">Thai</option>
            </select>
        </div>
        
        <div class="controls">
            <button id="connectBtn" class="primary" onclick="toggleConnection()">Connect</button>
            <button id="recordBtn" class="secondary" onclick="toggleRecording()" disabled>
                üé§ Start Recording
            </button>
            <button class="secondary" onclick="clearConversation()">Clear</button>
        </div>
        
        <div class="conversation" id="conversation">
            <div class="message assistant">
                <strong>Assistant:</strong> Welcome to LeafLoaf! I'm your voice-enabled grocery shopping assistant. 
                Click "Connect" to start, then use the microphone or type your questions.
            </div>
        </div>
        
        <div class="text-input">
            <input type="text" id="textInput" placeholder="Or type your message here..." disabled>
            <button class="primary" onclick="sendText()" disabled id="sendBtn">Send</button>
        </div>
        
        <div class="features">
            <h3>Voice Features</h3>
            <ul>
                <li><strong>Multi-lingual Support:</strong> Speak in your preferred language</li>
                <li><strong>Voice-Aware Responses:</strong> Adapts to your speaking style and emotion</li>
                <li><strong>Natural Conversation:</strong> Interrupt, clarify, or change topics naturally</li>
                <li><strong>Context Awareness:</strong> Remembers your preferences and conversation history</li>
            </ul>
            <p><strong>Try saying:</strong></p>
            <ul>
                <li>"Show me organic milk"</li>
                <li>"I need ingredients for pasta" (try in Italian!)</li>
                <li>"Add 5 bananas to my cart"</li>
                <li>"What's on sale today?"</li>
                <li>"‡§Æ‡•Å‡§ù‡•á ‡§¨‡§æ‡§∏‡§Æ‡§§‡•Ä ‡§ö‡§æ‡§µ‡§≤ ‡§ö‡§æ‡§π‡§ø‡§è" (Hindi)</li>
                <li>"Necesito tortillas y salsa" (Spanish)</li>
            </ul>
        </div>
    </div>

    <script>
        let ws = null;
        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;
        let audioContext = null;

        function updateStatus(message, type = 'info') {
            const status = document.getElementById('status');
            status.textContent = message;
            status.className = 'status ' + type;
        }

        function addMessage(content, type = 'assistant', metadata = null) {
            const conversation = document.getElementById('conversation');
            const message = document.createElement('div');
            message.className = 'message ' + type;
            
            let html = `<strong>${type === 'user' ? 'You' : 'Assistant'}:</strong> ${content}`;
            
            if (metadata) {
                html += `<div class="metadata">`;
                if (metadata.confidence) html += `Confidence: ${(metadata.confidence * 100).toFixed(1)}% | `;
                if (metadata.language) html += `Language: ${metadata.language} | `;
                if (metadata.intent) html += `Intent: ${metadata.intent} | `;
                if (metadata.execution_time) html += `Time: ${metadata.execution_time.toFixed(0)}ms`;
                html += `</div>`;
            }
            
            message.innerHTML = html;
            conversation.appendChild(message);
            conversation.scrollTop = conversation.scrollHeight;
        }

        async function toggleConnection() {
            const btn = document.getElementById('connectBtn');
            
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.close();
                btn.textContent = 'Connect';
                updateStatus('Disconnected', 'disconnected');
                document.getElementById('recordBtn').disabled = true;
                document.getElementById('textInput').disabled = true;
                document.getElementById('sendBtn').disabled = true;
            } else {
                connect();
            }
        }

        function connect() {
            const language = document.getElementById('language').value;
            const wsUrl = `ws://localhost:8000/api/v1/voice/google/ws?language=${language}`;
            
            updateStatus('Connecting...', 'processing');
            
            ws = new WebSocket(wsUrl);
            
            ws.onopen = () => {
                updateStatus('Connected', 'connected');
                document.getElementById('connectBtn').textContent = 'Disconnect';
                document.getElementById('recordBtn').disabled = false;
                document.getElementById('textInput').disabled = false;
                document.getElementById('sendBtn').disabled = false;
            };
            
            ws.onmessage = async (event) => {
                const data = JSON.parse(event.data);
                
                switch (data.type) {
                    case 'session_started':
                        addMessage(data.message);
                        break;
                        
                    case 'transcript':
                        if (data.is_final) {
                            addMessage(data.text, 'user', {
                                confidence: data.confidence,
                                language: data.language
                            });
                        }
                        break;
                        
                    case 'processing':
                        updateStatus(`Processing ${data.intent} request...`, 'processing');
                        break;
                        
                    case 'response':
                        addMessage(data.text, 'assistant', data.metadata);
                        updateStatus('Connected', 'connected');
                        break;
                        
                    case 'audio_response':
                        // Play audio response
                        playAudioResponse(data.audio, data.format);
                        break;
                        
                    case 'error':
                        addMessage(data.error, 'error');
                        updateStatus('Error occurred', 'disconnected');
                        break;
                        
                    case 'heartbeat':
                        // Keep connection alive
                        break;
                }
            };
            
            ws.onerror = (error) => {
                console.error('WebSocket error:', error);
                updateStatus('Connection error', 'disconnected');
            };
            
            ws.onclose = () => {
                updateStatus('Disconnected', 'disconnected');
                document.getElementById('connectBtn').textContent = 'Connect';
                document.getElementById('recordBtn').disabled = true;
                document.getElementById('textInput').disabled = true;
                document.getElementById('sendBtn').disabled = true;
            };
        }

        async function toggleRecording() {
            if (!isRecording) {
                startRecording();
            } else {
                stopRecording();
            }
        }

        async function startRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Create audio context for processing
                audioContext = new AudioContext({ sampleRate: 16000 });
                const source = audioContext.createMediaStreamSource(stream);
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                processor.onaudioprocess = (e) => {
                    if (!isRecording) return;
                    
                    const inputData = e.inputBuffer.getChannelData(0);
                    const outputData = new Int16Array(inputData.length);
                    
                    // Convert float32 to int16
                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        outputData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                    }
                    
                    // Send audio chunk
                    if (ws && ws.readyState === WebSocket.OPEN) {
                        ws.send(JSON.stringify({
                            type: 'audio',
                            audio: btoa(String.fromCharCode(...new Uint8Array(outputData.buffer)))
                        }));
                    }
                };
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                isRecording = true;
                document.getElementById('recordBtn').textContent = '‚èπÔ∏è Stop Recording';
                document.getElementById('recordBtn').className = 'recording';
                updateStatus('Recording... Speak now', 'processing');
                
            } catch (error) {
                console.error('Error accessing microphone:', error);
                addMessage('Error: Could not access microphone. Please check permissions.', 'error');
            }
        }

        function stopRecording() {
            isRecording = false;
            
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }
            
            // Signal end of stream
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: 'end_stream' }));
            }
            
            document.getElementById('recordBtn').textContent = 'üé§ Start Recording';
            document.getElementById('recordBtn').className = 'secondary';
            updateStatus('Processing...', 'processing');
        }

        function sendText() {
            const input = document.getElementById('textInput');
            const text = input.value.trim();
            
            if (text && ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: 'text',
                    text: text
                }));
                
                addMessage(text, 'user');
                input.value = '';
                updateStatus('Processing...', 'processing');
            }
        }

        function changeLanguage() {
            const language = document.getElementById('language').value;
            
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: 'config',
                    config: { language: language }
                }));
                
                addMessage(`Language changed to ${language}`, 'assistant');
            }
        }

        async function playAudioResponse(audioBase64, format) {
            try {
                const audioData = atob(audioBase64);
                const arrayBuffer = new ArrayBuffer(audioData.length);
                const view = new Uint8Array(arrayBuffer);
                
                for (let i = 0; i < audioData.length; i++) {
                    view[i] = audioData.charCodeAt(i);
                }
                
                const audioContext = new AudioContext();
                const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
                
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);
                source.start();
                
            } catch (error) {
                console.error('Error playing audio:', error);
            }
        }

        function clearConversation() {
            const conversation = document.getElementById('conversation');
            conversation.innerHTML = `
                <div class="message assistant">
                    <strong>Assistant:</strong> Conversation cleared. How can I help you?
                </div>
            `;
        }

        // Handle Enter key in text input
        document.getElementById('textInput').addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                sendText();
            }
        });

        // Auto-connect on load
        window.addEventListener('load', () => {
            // Uncomment to auto-connect
            // setTimeout(connect, 500);
        });
    </script>
</body>
</html>